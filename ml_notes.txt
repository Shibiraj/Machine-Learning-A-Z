Part 1 - Data Preprocessing
	Imputer is for filling the nan values
		from sklearn.preprocessing import Imputer
		imputer = Imputer(missing_values="NaN", strategy="mean", axis=0)
		imputer = imputer.fit(X[:, 1:3])
		X[:, 1:3] = imputer.transform(X[:, 1:3])	
		OR
		df[dimensions] = df[dimensions].fillna(value="")


	Dummy encoding : Encoding Germany, France, Spain to 3 columns (LabelEncoder and OneHotEncoder)
		# Encoding categorical day 20
		# dummy encoding France having higher value than Germany to avoid that we use OneHotEncoder 1 0 0 (france)

		represent the column in another format

	splitting dataset: 
		from sklearn.cross_validation import train_test_split
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

	Feature scaling
		from sklearn.preprocessing import StandardScaler
		sc_X = StandardScaler()
		X_train = sc_X.fit_transform(X_train)
		X_test = sc_X.transform(X_test)



Regression
	Simple Linear Regression
		y = mx + c (y = DV, x = IV, c = constant or bias term., m = coefficient or slope of the line)
		salary experience problem
			salary = m * experience + c
		Find the best fit line (The line for which the the error between the predicted values and the observed values is minimum is called the best fit line or the regression line. These errors are also called as residuals.) 

		Gradient descent

			Gradient descent is a generic optimization algorithm used in many machine learning algorithms. It iteratively tweaks the parameters of the model in order to minimize the cost function. The steps of gradient descent is outlined below.



COde
For generating random dataset
	np.random.seed(0)
	x = np.random.rand(100, 1)
	y = 2 + 3 * x + np.random.rand(100, 1)